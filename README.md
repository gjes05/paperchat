# ğŸ§  PaperChat â€” RAG-Powered Document Q&A

PaperChat lets you upload PDF documents and have a grounded, conversational Q&A session with them â€” powered by LangChain, Gemini 2.5 Flash, Pinecone, and Streamlit.

---

## ğŸ—ï¸ Architecture

```
PDF Upload â†’ PyPDF Loader â†’ Text Splitter (RecursiveCharacter)
    â†’ Pinecone Llama Embeddings â†’ Pinecone Vector Store
         â†“
User Question â†’ Similarity Search (Top-K chunks)
    â†’ Gemini 2.5 Flash + Conversation Memory â†’ Grounded Answer + Sources
```

## ğŸ—‚ï¸ Project Structure

```
PaperChat/
â”œâ”€â”€ app.py              # Streamlit frontend
â”œâ”€â”€ rag_pipeline.py     # Core RAG logic (ingest + query)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/yourusername/PaperChat.git
cd PaperChat
pip install -r requirements.txt
```

### 2. Set Up API Keys

```bash
cp .env.example .env
```

Edit `.env` with your keys:

| Variable | Where to get it |
|---|---|
| `GOOGLE_API_KEY` | [Google AI Studio](https://aistudio.google.com/app/apikey) |
| `PINECONE_API_KEY` | [Pinecone Console](https://app.pinecone.io/) |
| `PINECONE_INDEX_NAME` | Choose any name, e.g. `paperchat` |

### 3. Run

```bash
streamlit run app.py
```

Open [http://localhost:8501](http://localhost:8501) in your browser.

---

## âœ¨ Features

- **Multi-document support** â€” upload and query across multiple PDFs simultaneously
- **Conversational memory** â€” follow-up questions use context from previous turns (sliding window of 5 turns)
- **Source citations** â€” every answer shows which file and page the information came from
- **Grounded answers** â€” the model is instructed to only answer from the document context
- **Clean UI** â€” dark-mode Streamlit interface with custom CSS

## ğŸ”§ Configuration

Key parameters in `rag_pipeline.py`:

| Constant | Default | Description |
|---|---|---|
| `CHUNK_SIZE` | 1000 | Characters per chunk |
| `CHUNK_OVERLAP` | 200 | Overlap between chunks |
| `TOP_K` | 5 | Retrieved chunks per query |
| `CHAT_MODEL` | `gemini-2.5-flash` | LLM for answer generation |
| `EMBEDDING_MODEL` | `llama-text-embed-v2` | Pinecone embedding model |

## ğŸ“Š Evaluation Metrics (for extending the project)

- **Faithfulness**: Does the answer only use info from context? 
- **Answer Relevancy**: Is the answer relevant to the question?
- **Context Recall**: Are the right chunks being retrieved?

```bash
pip install ragas
```

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| LLM | Gemini 2.5 Flash (Google) |
| Embeddings | Pinecone `llama-text-embed-v2` (1024-dim) |
| Vector DB | Pinecone (serverless) |
| Orchestration | LangChain |
| Frontend | Streamlit |
| PDF Parsing | PyPDF |